'''Author: Namas Bhandari
   Date: 24/01/25
   This script defines an Encoder block that takes the patch embeddings generated by the PatchEmbedding module from the input image. 
   It also defines the entire Encoder architecture for the Transformer.
   These patch embeddings are passed into the Encoder to obtain the outputs that need to be passed to the decoder.
'''

import torch.nn as nn
import torch
from attention import MultiHeadedAttention

class EncoderBlock(nn.Module):
    def __init__(self, hidden_size, embedding_size=512, number_of_heads=8):
        super().__init__()
        assert embedding_size % number_of_heads == 0, "Embedding size must be divisible by the number of heads."
        self.embedding_size = embedding_size
        self.number_of_heads = number_of_heads
        self.hidden_size = hidden_size # Hidden size for the Feed-Forward Network

        # Intialize the layers of the Encoder block
        self.mha = MultiHeadedAttention(self.embedding_size, self.number_of_heads)
        self.layer_norm1 = nn.LayerNorm(self.embedding_size)
        self.ffn = nn.Sequential(
            nn.Linear(self.embedding_size, self.hidden_size),
            nn.GELU(),
            nn.Linear(self.hidden_size, self.embedding_size)
        )
        self.layer_norm2 = nn.LayerNorm(self.embedding_size)
    
    def forward(self, patch_embeddings):
    
        # First, we pass the patch embeddings through the MultiHeadAttention module.
        mha_activations = self.mha(patch_embeddings) # (BatchSize, NumberOfPatches, EmbeddingSize) -> (BatchSize, NumberOfPatches, EmbeddingSize)

        # Now, we apply LayerNormalization on the sequence and add the original patch embeddings to it, to create a skip connection.
        linear_input = mha_activations + patch_embeddings # (BatchSize, NumberOfPatches, EmbeddingSize) -> (BatchSize, NumberOfPatches, EmbeddingSize)
        normalized_linear_input = self.layer_norm1(linear_input) # (BatchSize, NumberOfPatches, EmbeddingSize) -> (BatchSize, NumberOfPatches, EmbeddingSize)

        # Now, we pass it through the Linear layer and add the normalized activations from the LayerNorm layer, then apply another LayerNorm on the output.
        out = self.ffn(normalized_linear_input) # (BatchSize, NumberOfPatches, EmbeddingSize) -> (BatchSize, NumberOfPatches, EmbeddingSize)
        out += normalized_linear_input # (BatchSize, NumberOfPatches, EmbeddingSize) -> (BatchSize, NumberOfPatches, EmbeddingSize)
        out = self.layer_norm2(out) # (BatchSize, NumberOfPatches, EmbeddingSize) -> (BatchSize, NumberOfPatches, EmbeddingSize)

        return out
    
class Encoder(nn.Module):
    def __init__(self, embedding_size=512, n_blocks=8, number_of_heads=8, hidden_size_multiplier=4):
        super().__init__()
        assert embedding_size % number_of_heads == 0, "Embedding size must be divisible by the number of heads."
        self.embedding_size = embedding_size
        self.number_of_heads = number_of_heads
        self.hidden_size = embedding_size * hidden_size_multiplier # Hidden size for the Feed-Forward Network is generally 4 times the embedding_size.
        self.n_blocks = n_blocks

        # We instantiate n_blocks Encoder blocks within the Encoder.
        self.encoder_blocks = nn.ModuleList([EncoderBlock(hidden_size=self.hidden_size, embedding_size=self.embedding_size, number_of_heads=self.number_of_heads) for i in range(self.n_blocks)])

    def forward(self, patch_embeddings):
        for encoder_block in self.encoder_blocks:
            patch_embeddings = encoder_block(patch_embeddings)        
        return patch_embeddings

if __name__ == '__main__':
    embedding_size = 512
    number_of_heads = 8
    num_patches =  32
    hidden_size_multiplier = 4
    hidden_size = embedding_size * hidden_size_multiplier
    patch_embeddings = torch.randn((3, num_patches, embedding_size))

    print(f'----- Testing single Encoder block -----')
    encoder_block = EncoderBlock(hidden_size=hidden_size, embedding_size=embedding_size, number_of_heads=number_of_heads)
    encoder_block_output = encoder_block(patch_embeddings)
    print(f'Patch Embeddings with shape:{patch_embeddings.shape} processed through an Encoder Block output:{encoder_block_output.shape}')
    print(f'----- Encoder block test finished successfully -----')


    print(f'----- Testing entire Encoder -----')
    n_blocks = 8
    encoder = Encoder(embedding_size=embedding_size, n_blocks=n_blocks, number_of_heads=number_of_heads, hidden_size_multiplier=hidden_size_multiplier)
    encoder_output = encoder(patch_embeddings)
    print(f'Patch Embeddings with shape:{patch_embeddings.shape} processed through an entire Encoder output:{encoder_output.shape}')
    print(f'----- Encoder test finished successfully -----')

    